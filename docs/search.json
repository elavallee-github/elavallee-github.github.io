[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my super cool about section of my blog"
  },
  {
    "objectID": "posts/logistic-blog/LogisticRegressionBlog.html",
    "href": "posts/logistic-blog/LogisticRegressionBlog.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "logistic.py\n\n\nAbstract\nThe goal of this blog post is to investigate an implementation of a logistic regression model. By running different datasets through the main training loop found later in this blog post we can investigate how a logistic regression model performs at classifying data and how the classification speed can be increased through a technique called momentum. We can also explore how the model performs when the dataset has more features than there are data points.\n\n\nPart A: Implement Logistic Regression\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nimport torch\nfrom matplotlib import pyplot as plt\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(noise = 0.5)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n\nVisualization Code\nTaken from live notes by Professor Chodrow\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n\nMain Training Loop\nThe following code will be used to streamline testing of different learning rates and how momentum affects the efficiency of logistic regression model training. One plot will be produced to visualize the dividing line in the data and another plot will be used to visualize the loss value over the iterations.\n\ndef train_LR(X, y, alpha, beta):\n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    loss = 1\n    loss_vec = []\n    weights = []\n    w_prev = None\n\n    for _ in range(100):\n        # add other stuff to e.g. keep track of the loss over time. \n        loss = LR.loss(X,y)\n        loss_vec.append(loss)\n        if w_prev == None:\n            w_prev = LR.w\n        w = LR.w\n        weights.append(w)\n        opt.step(X, y, alpha = alpha, beta = beta, w_prev=w_prev)\n        w_prev = w\n\n    fig, ax = plt.subplots(1, 2, figsize=(10,5))\n    plot_classification_data(X, y, ax[0])\n    draw_line(LR.w, -1, 2, ax[0], color = \"black\")\n    ax[1].plot(loss_vec, color = \"slategrey\")\n    ax[1].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n    labs = plt.gca().set(xlabel = \"Logistic Regression Iteration (Updates Only)\", ylabel = \"loss\")\n    \n\n\ntrain_LR(X, y, alpha=0.1, beta=0.0)\n\n\n\n\n\n\n\n\nPlot 1: Logistic Regression training using a learning rate of 0.1 and without momentum\n\ntrain_LR(X, y, alpha=0.5, beta=0.9)\n\n\n\n\n\n\n\n\nPlot 2: Logistic Regression results using a learning rate of 0.5 and a momentum with a beta value of 0.9\nLooking at the above plots (plots 1 and 2) we can see that visually my implementation of the logistic regression model is correct in that both plots have a dividing line that seemingly divides the data correctly between the two groups. We also see a trend we were hoping to observe where when we use momentum we see a decrease in the number of iterations it takes for us to find that dividing line and level out in our loss calculations.\n\n\n\nTraining Models with Dimensions Larger than their Points\n\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\nBelow we are training a logistic regression model on our X_train and y_train data points\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss = 1\nloss_vec = []\nweights = []\nw_prev = None\n\nfor _ in range(100):\n    # add other stuff to e.g. keep track of the loss over time. \n    loss = LR.loss(X_train,y_train)\n    loss_vec.append(loss)\n    if w_prev == None:\n        w_prev = LR.w\n    w = LR.w\n    weights.append(w)\n    opt.step(X_train, y_train, alpha = 0.5, beta = 0.9, w_prev=w_prev)\n    w_prev = w\n\n\n(1.0*(LR.predict(X_train)==y_train)).mean()\n\ntensor(1.)\n\n\nAs we can see from the above output we have successfully trained a logistic regression model that has 100% prediction accuracy on the training data\n\n(1.0*(LR.predict(X_test) == y_test)).mean()\n\ntensor(0.9600)\n\n\nHowever looking at the prediction accuracy on the testing data we can see it is 96% which is still pretty good, but it is not as good as our training data which leads me to believe there is an element of overfitting going on in the training data.\n\n\nDiscussion\nDuring this blog post we experimented on the prediction capabilities of my implementation of a logistic regression model. By running multiple datasets through the training loop I coded earlier in this blog we could see how the learning rate of a model as well as whether or not momentum is used affects a models classification abilities. From my experimenting we saw that by using a learning rate of 0.5 and a momentum learning rate of 0.9 we can decrease the number of training loop iterations it takes for a model to reach a low loss score. Without momentum it would take the model close to 100 iterations to reach a loss score close to 0, however with the use of momentum we could decrease that to around 20 iterations. Additionally through my testing we saw that we could train a model to 100% prediction accuracy on its training data even if there are more features than there are data points. However, there is a risk of overfitting the training data as in the case seen above we have 100% prediction accuracy on the training data but only 96% on the test data. Overall it seems that a logistic regression model isn’t the most complicated algorithm to implement and is efficient at reducing the number of iterations it takes for a model to be trained."
  },
  {
    "objectID": "posts/penguin-blog/PenguinBlog.html",
    "href": "posts/penguin-blog/PenguinBlog.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Abstract:\nThe goal of this exploration is to train a model (Linear Regression or other) that can predict the species of a penguin from the palmer penguins dataset with 100% accuracy using 1 qualitative and 2 quantitative features. To begin this exploration, I filtered the data to allow for easier training to be completed. At the same time as filtering the data, I did some preliminary investigations into what combination of qualitative and quantitative features would be lead to predicting the correct penguin species. That investigation was done via grouping and aggregating the dataset and creating graphs comparing features. To actually confirm which combination of features would best predict penguin species I used the provided code from Professor Chodrow to loop through all possible feature combinations and check cross validation and training scores of each. I compiled that data into a dataframe and chose the set of features with the highest scores, that turned out to be island, culmen length, and culmen depth. That combination of features produced a Linear Regression model with 100% accuracy on the testing dataset.\nImport training set and convert it to a pandas dataframe\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nFilter training data, converting the “Species” column into integer values to better predict on during training process. Also split up training data into a DataFrame of features and a list of the “Species” in integer form.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nFeature exploration:\nFigures produced comparing two quantitative features from the training data against one qualitative one. Following that is a grouping of the dataset specifically looking at mean Flipper Length of the different species compared to sex.\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize = (11, 6))\nx1='Body Mass (g)'\ny1='Culmen Length (mm)'\nz1='Species'\nsns.scatterplot(ax=ax[0], data=train[[z1, x1, y1]], x=x1, y=y1, hue=z1)\n\nx2='Flipper Length (mm)'\ny2='Body Mass (g)'\nz2='Species'\nsns.scatterplot(ax=ax[1], data=train[[z2, x2, y2]], x=x2, y=y2, hue=z2)\n\n\n\n\n\n\n\n\n\ntrain.loc[train['Sex'] != '.'].groupby(['Species', 'Sex']).aggregate({'Flipper Length (mm)' : 'mean'})\n\n\n\n\n\n\n\n\n\nFlipper Length (mm)\n\n\nSpecies\nSex\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n187.924528\n\n\nMALE\n192.327869\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n192.064516\n\n\nMALE\n200.692308\n\n\nGentoo penguin (Pygoscelis papua)\nFEMALE\n212.836735\n\n\nMALE\n221.204545\n\n\n\n\n\n\n\n\n\nMain Training Loop:\nThe following code is the main training loop, going over all possible combinations of the chosen quantitative and qualitative features. Each qualitative feature is paired with two quantitative ones and then a linear regression model is trained on those. The cross validation an score of the model is calculated and appended to a list along with the feature combinations to keep track of which features score the best.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression(max_iter=5000)\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\nLR_scores = []\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols\n        print(cols)\n        LR.fit(X_train[cols], y_train)\n        cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n        score = LR.score(X_train[cols], y_train)\n        LR_scores.append([cv_scores_LR.mean(), score] + cols)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Flipper Length (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Flipper Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Flipper Length (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nConvert the LR_scores list into a DataFrame for data analysis\n\nLR_df = pd.DataFrame(LR_scores)\nLR_df.columns = ['CV_score', 'Score', 'Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']\nLR_df.sort_values('CV_score', ascending=False).head(5)\n\n\n\n\n\n\n\n\nCV_score\nScore\nFeature 1\nFeature 2\nFeature 3\nFeature 4\nFeature 5\n\n\n\n\n6\n0.988311\n0.996094\nCulmen Length (mm)\nCulmen Depth (mm)\nSex_FEMALE\nSex_MALE\nNone\n\n\n12\n0.984389\n0.996094\nCulmen Length (mm)\nCulmen Depth (mm)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\n\n\n8\n0.972700\n0.980469\nCulmen Length (mm)\nBody Mass (g)\nSex_FEMALE\nSex_MALE\nNone\n\n\n14\n0.972624\n0.976562\nCulmen Length (mm)\nBody Mass (g)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\n\n\n7\n0.968854\n0.980469\nCulmen Length (mm)\nFlipper Length (mm)\nSex_FEMALE\nSex_MALE\nNone\n\n\n\n\n\n\n\nAfter looking at the top scores the rows with the best cross validation and training score were analyzed, and while culmen length and depth compared with sex had a slightly higher mean cv_score, I chose to go with the second best score of culmen length/depth compared with island as I thought it would produce more interesting plots. Once the features are selected from that process they are used to retrain a linear regression model.\n\nmax_cols = LR_df.iloc[12]\ncols = max_cols[2:]\nLR.fit(X_train[cols], y_train)\n\nLogisticRegression(max_iter=5000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=5000)\n\n\nImport testing data and prepare it then calculate a score on the data using LR model that was trained in the last step.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nAs can be seen above a testing score of 1.0 or 100% was achieved from this model\n\n\nData Visualization\nThe following code is copied from Professor Chodrow it is used to visual the results from model training.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nPlot regions and visualize model prediction success. Following that a confusion matrix was produced to further show model prediction rates.\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nLooking at the above confusion matrix we are correctly predicting all of the categories we possibly can. The first column is predicted Biscoe islands, the second predicted Dream, and third predicted Torgersen and as we can see all the islands were correctly predicted.\n\n\nDiscussion:\nLooking back at the results from this experimentation, we can see that when working with the palmer penguins dataset we can produced a linear regression model that predicts the species of a test set that hasn’t been seen before with 100% accuracy when the model is trained using Island, Culmen Length, and Culmen Depth. However, I personally am very skeptical of a model that can predict with 100% accuracy, no amount of features will ever be a perfect calculator for predicting something and I hypothesize that if the palmer penguins dataset was larger and there were more outliers we wouldn’t see such a high training score. That being said, it is cool to see how we can train a model to pattern recognize and we can be smart about what patterns we guide our model to recognize in order to best set up its predicting capabilities."
  },
  {
    "objectID": "posts/WhoseCosts-blog/OptimalDecisionMaking.html",
    "href": "posts/WhoseCosts-blog/OptimalDecisionMaking.html",
    "title": "Who’s Cost?",
    "section": "",
    "text": "Abstract\nThe goals of this experiment is to examine thresholding on a dataset focussing on loan defaulting and how that can be down to maximize the gain of a bank when deciding to make loans using a machine learning model. To complete this experiment plots were initially made to find possible features of the dataset to conduct model training on. Once features were found, a logistic regression model was trained and its model weights were used to conduct experimenting to find the ideal threshold for bank gain maximization. The weight was used to calculate a linear score which produced scores which aided in thresholding. An ROC curve was created to visualize the effectiveness of the model, and then the gain was calculated using the TNR and the FNR. Through this the gain was found to be about 8million with a weight of [-0.05043871, 0.27511746] and a threshold of 5.8.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\n\ndf_all\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\nMORTGAGE\n8.0\nEDUCATION\nA\n3000\n7.29\n0\n0.02\nN\n17\n\n\n26060\n23\n48000\nRENT\n1.0\nVENTURE\nA\n4325\n5.42\n0\n0.09\nN\n4\n\n\n26061\n22\n60000\nRENT\n0.0\nMEDICAL\nB\n15000\n11.71\n0\n0.25\nN\n4\n\n\n26062\n30\n144000\nMORTGAGE\n12.0\nPERSONAL\nC\n35000\n12.68\n0\n0.24\nN\n8\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n7.29\n1\n0.36\nN\n4\n\n\n\n\n26064 rows × 12 columns\n\n\n\n\ndf_all.groupby(['loan_intent']).aggregate({'person_age':'mean', 'person_emp_length':'mean'})\n\n\n\n\n\n\n\n\nperson_age\nperson_emp_length\n\n\nloan_intent\n\n\n\n\n\n\nDEBTCONSOLIDATION\n27.588798\n4.759419\n\n\nEDUCATION\n26.597620\n4.440192\n\n\nHOMEIMPROVEMENT\n28.981737\n5.103754\n\n\nMEDICAL\n27.950982\n4.782062\n\n\nPERSONAL\n28.288339\n4.897997\n\n\nVENTURE\n27.588643\n4.877869\n\n\n\n\n\n\n\nLooking at the above dataframe we can see there isn’t really a stark correlation between loan intent, age, and employment length. I think the only conclusion that could be drawn from this is that mean employment length can slightly be determined but even that is a stretch\n\n\nFeature Exploration\nThe following code aims to explore possible classification features in the dataset used in this experiment\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nsns.scatterplot(data=df_all[['loan_percent_income', 'loan_int_rate', 'cb_person_default_on_file']], x='loan_int_rate', y='loan_percent_income', hue='cb_person_default_on_file')\n\n\n\n\n\n\n\n\nIn the above plot we can see that as there is a relationship between loan interest rate, loan percent income, and someone’s likelihood to have defaulted on a loan. It seems that loan interest rate is the larger factor seeing as regardless of loan percent income, there is an abrupt increase in load defaults as loan interest rate reaches a level of 12.5 or above.\n\nfig, axs = plt.subplots(1, 1, figsize=(5,3))\n\nsns.scatterplot(ax=axs, data=df_all[['person_income', 'loan_amnt', 'loan_status']], x='loan_amnt', y='person_income', hue='loan_status')\naxs.set_ylim(0,250000)\n\n\n\n\n\n\n\n\nLooking at the above graph there seems to be a slight trend that there is a relationship between loan amount, person income, and whether or not someone has defaulted on their loan. There is a heavey concentration of loan defaults for persons with an income of $50,000 or lower and this trend maintains for loans less than $5000 and up to loans of $35000.\n\ndf_all[\"gain_if_repaid\"] = df_all['loan_amnt']*(1 + df_all['loan_int_rate']/100)**10 - df_all['loan_amnt']\ndf_all[\"cost_if_default\"] = df_all['loan_amnt']*(1 + df_all['loan_int_rate']/100)**5 - 1.5*df_all['loan_amnt']\ndf_all\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\ngain_if_repaid\ncost_if_default\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n1887.118673\n124.718787\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n29826.546771\n4477.588639\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n10629.496036\n-637.028150\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n3121.324231\n439.716432\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n22617.107668\n1254.086280\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\nMORTGAGE\n8.0\nEDUCATION\nA\n3000\n7.29\n0\n0.02\nN\n17\n3063.364932\n-235.015264\n\n\n26060\n23\n48000\nRENT\n1.0\nVENTURE\nA\n4325\n5.42\n0\n0.09\nN\n4\n3006.894895\n-856.297160\n\n\n26061\n22\n60000\nRENT\n0.0\nMEDICAL\nB\n15000\n11.71\n0\n0.25\nN\n4\n30395.392679\n3594.652521\n\n\n26062\n30\n144000\nMORTGAGE\n12.0\nPERSONAL\nC\n35000\n12.68\n0\n0.24\nN\n8\n80487.884885\n11077.322773\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n7.29\n1\n0.36\nN\n4\n21903.059263\n-1680.359140\n\n\n\n\n26064 rows × 14 columns\n\n\n\nThe above code is calculating gain if repaid and cost if default based on an interest rate calculation supplied by Professor Chodrow. Those values will be used later on in the experiment we we look into thresholding and how our model predictions will go into loss and gain calculations.\n\ncols = ['person_emp_length', 'loan_int_rate', 'loan_status', 'gain_if_repaid', 'cost_if_default']\ndf = df_all[cols]\ndf = df.dropna()\ndf\n\n\n\n\n\n\n\n\nperson_emp_length\nloan_int_rate\nloan_status\ngain_if_repaid\ncost_if_default\n\n\n\n\n1\n3.0\n13.47\n0\n29826.546771\n4477.588639\n\n\n2\n5.0\n7.51\n0\n10629.496036\n-637.028150\n\n\n3\n2.0\n12.87\n1\n3121.324231\n439.716432\n\n\n4\n2.0\n9.63\n0\n22617.107668\n1254.086280\n\n\n6\n2.0\n14.91\n1\n16577.044649\n2769.244329\n\n\n...\n...\n...\n...\n...\n...\n\n\n26059\n8.0\n7.29\n0\n3063.364932\n-235.015264\n\n\n26060\n1.0\n5.42\n0\n3006.894895\n-856.297160\n\n\n26061\n0.0\n11.71\n0\n30395.392679\n3594.652521\n\n\n26062\n12.0\n12.68\n0\n80487.884885\n11077.322773\n\n\n26063\n5.0\n7.29\n1\n21903.059263\n-1680.359140\n\n\n\n\n22907 rows × 5 columns\n\n\n\nFor our experiment we are going to be looking into how person employment length and loan interest rate can be used to predict loan status\n\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size = 0.2)\n\n\nX_train = df_train[[\"person_emp_length\", \"loan_int_rate\"]]\ny_train = df_train[\"loan_status\"]\n\nX_test = df_test[[\"person_emp_length\", \"loan_int_rate\"]]\ny_test = df_test[\"loan_status\"]\n\nHere we are splitting the data into training and testing data and then further splitting it into a training list of just features and a list of targets, those being X_train and y_train respectively.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nLR = LogisticRegression()\n\nm = LR.fit(X_train, y_train)\nLR.score(X_train, y_train)\n\n0.8018008185538882\n\n\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv = 5)\ncv_scores_LR.mean()\n\n0.8019645293315143\n\n\nWe now train a Logistic Regression model using our training data and conduct a cross validation on that. We get a training score and a cross validation score of ~80 which isn’t great, but it is good enough to continue with our experiment\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nw = LR.coef_[0]\nw\n\narray([-0.04834875,  0.27540715])\n\n\nWe trained a logistic regression model above in order to get a weight value from the model using the above code LR.coef_[0]. This is done so we can use the weight along with our X_train data to create scores for our target values.\n\n\nScore Calculation\nCan divide scores by t to put them between 0-1\n\ndef linear_score(X, w):\n    return X@w\n\n\ns = linear_score(X_train, w)\ndf['score'] = s\ndf.dropna()\ndf\n\n\n\n\n\n\n\n\nperson_emp_length\nloan_int_rate\nloan_status\ngain_if_repaid\ncost_if_default\nscore\n\n\n\n\n1\n3.0\n13.47\n0\n29826.546771\n4477.588639\n3.564688\n\n\n2\n5.0\n7.51\n0\n10629.496036\n-637.028150\n1.826564\n\n\n3\n2.0\n12.87\n1\n3121.324231\n439.716432\n3.447793\n\n\n4\n2.0\n9.63\n0\n22617.107668\n1254.086280\n2.555473\n\n\n6\n2.0\n14.91\n1\n16577.044649\n2769.244329\n4.009623\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n8.0\n7.29\n0\n3063.364932\n-235.015264\n1.620928\n\n\n26060\n1.0\n5.42\n0\n3006.894895\n-856.297160\n1.444358\n\n\n26061\n0.0\n11.71\n0\n30395.392679\n3594.652521\n3.225018\n\n\n26062\n12.0\n12.68\n0\n80487.884885\n11077.322773\n2.911978\n\n\n26063\n5.0\n7.29\n1\n21903.059263\n-1680.359140\nNaN\n\n\n\n\n22907 rows × 6 columns\n\n\n\nUsing the linear_score() function we created above we can create a list of scores and append that to our dataframe with both our cost if default and gain if repaid information as well.\n\nhist = plt.hist(s)\nlabs = plt.gca().set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nWe can plot a histogram of our scores to see where we might want to look for our threshold. We are trying to find a threshold that maximizes our prediction accuracy and that process can be seen in the code block below.\n\n\nThresholding\n\nfor t in np.linspace(0, 6, 11):\n    y_pred = s&gt;= t\n    acc    = (y_pred == y_train).mean()\n    print(f\"A threshold of {t:.1f} gives an accuracy of {acc:.2f}.\")\n\nA threshold of 0.0 gives an accuracy of 0.22.\nA threshold of 0.6 gives an accuracy of 0.22.\nA threshold of 1.2 gives an accuracy of 0.23.\nA threshold of 1.8 gives an accuracy of 0.36.\nA threshold of 2.4 gives an accuracy of 0.50.\nA threshold of 3.0 gives an accuracy of 0.65.\nA threshold of 3.6 gives an accuracy of 0.78.\nA threshold of 4.2 gives an accuracy of 0.81.\nA threshold of 4.8 gives an accuracy of 0.79.\nA threshold of 5.4 gives an accuracy of 0.79.\nA threshold of 6.0 gives an accuracy of 0.78.\n\n\n\nt = 4.2\ny_pred = s&gt;= t\n((y_train == 0) * (y_pred == 1)).sum() # calculating false positives\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_train, y_pred)\nconfusion_matrix(y_train, y_pred, normalize=\"true\")\n\narray([[0.97099534, 0.02900466],\n       [0.79533941, 0.20466059]])\n\n\nFrom looping through possible score thresholds we found that 4.2 gave an accuracy of 0.81 which is the highest we saw in our possible thresholds. We will get a more exact value later on but here we can use it to confirm our progress so far by creating a confusion matrix to see true positives, true negatives, false positives, and false negatives.\n\ndf['preds'] = y_pred\ndf['preds'].sum()\n\n1225\n\n\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\nnum_thresholds = 101\n\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\nT = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\ns    = linear_score(X_train, w)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds    = s &gt;= t\n    FPR[i]   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i]   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nax.plot(FPR, TPR, color = \"black\")\nax.plot([0,1], [0,1], linestyle=\"--\", color = \"grey\")\nax.set_aspect('equal')\n\nlabs = ax.set(xlabel = \"False Positive Rate\", ylabel = \"True Positive Rate\", title = \"ROC Curve\")\n\n\n\n\n\n\n\n\nAbove we are creating a plot of the TPR v FPR and how that changes as we use different threshold values.\n\n\nGain Calculation\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\ncost_of_FN = df.loc[df[\"preds\"] == False]['cost_if_default'].sum()\ngain_of_TN = df.loc[df[\"preds\"] == True]['gain_if_repaid'].sum()\n\ngain =  cost_of_FN*FNR  + gain_of_TN*TNR\n\nplt.plot(T, gain)\n#plt.gca().set(ylim = (), xlim = ())\nlabs = plt.gca().set(xlabel = r\"Threshold $t$\", ylabel = \"Expected profit per loan\")\n\n\n\n\n\n\n\n\nHere we are plotting the gain of our model using different threshold values. We can see that a threshold of around 4.5 is when our model starts to plateau and we don’t see any more increase to our profits. I don’t think this exactly what we’d like to see. Ideally there is a distinct spike that would point out where our ideal threshold should be, but I think the important piece here is that we see a trend similar to what we saw earlier in the experiment that thresholds less than 4 are not ideal.\n\nt = 5.8\n\n# compute the scores\ns     = linear_score(X_test, w)\npreds = s &gt;= t\n\n# compute error rates\nFPR   = ((preds == 1) & (y_test == 0)).sum() / (y_test == 0).sum()\nTPR   = ((preds == 1) & (y_test == 1)).sum() / (y_test == 1).sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = cost_of_FN*FNR  + gain_of_TN*TNR \ngain\n\n87232429.77910945\n\n\nHere we are calculating our max possible gain, which as we can see uses a threshold larger than what was first thought from the plot above, but through this calculation we can see that the max gain we can get from a threshold of 5.8 is ~ $8.7 million which feels quite high, but is the calculation we are getting.\n\n\nGain Calculation on Test Dataset\nFor the following data we are going to follow a very similar process to that of what was accomplished above. We are going to use our previously calculated weight, compute scores for the dataset, gain and cost values, then calculate a total gain, and a gain per loan for the dataset.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\n\ndf_test\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n21\n42000\nRENT\n5.0\nVENTURE\nD\n1000\n15.58\n1\n0.02\nN\n4\n\n\n1\n32\n51000\nMORTGAGE\n2.0\nDEBTCONSOLIDATION\nB\n15000\n11.36\n0\n0.29\nN\n9\n\n\n2\n35\n54084\nRENT\n2.0\nDEBTCONSOLIDATION\nC\n3000\n12.61\n0\n0.06\nN\n6\n\n\n3\n28\n66300\nMORTGAGE\n11.0\nMEDICAL\nD\n12000\n14.11\n1\n0.15\nN\n6\n\n\n4\n22\n70550\nRENT\n0.0\nMEDICAL\nE\n7000\n15.88\n1\n0.08\nN\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6512\n26\n26000\nMORTGAGE\n4.0\nHOMEIMPROVEMENT\nB\n12000\nNaN\n0\n0.46\nN\n3\n\n\n6513\n27\n44640\nRENT\n0.0\nMEDICAL\nB\n12800\n11.83\n0\n0.29\nN\n9\n\n\n6514\n24\n48000\nOWN\n5.0\nVENTURE\nA\n10400\n7.37\n0\n0.22\nN\n3\n\n\n6515\n26\n65000\nMORTGAGE\n6.0\nEDUCATION\nA\n6000\n9.07\n0\n0.09\nN\n3\n\n\n6516\n29\n61000\nRENT\n12.0\nVENTURE\nD\n10000\n16.07\n0\n0.16\nN\n9\n\n\n\n\n6517 rows × 12 columns\n\n\n\n\ndf_test[\"gain_if_repaid\"] = df_test['loan_amnt']*(1 + df_test['loan_int_rate']/100)**10 - df_test['loan_amnt']\ndf_test[\"cost_if_default\"] = df_test['loan_amnt']*(1 + df_test['loan_int_rate']/100)**5 - 1.5*df_test['loan_amnt']\n\nAbove is the gain if repaid and cost if default calculations that we saw previously\n\nX_test = df_test[cols[:2]]\ny_test = df_test[cols[2]]\nX_test\n\n\n\n\n\n\n\n\nperson_emp_length\nloan_int_rate\n\n\n\n\n0\n5.0\n15.58\n\n\n1\n2.0\n11.36\n\n\n2\n2.0\n12.61\n\n\n3\n11.0\n14.11\n\n\n4\n0.0\n15.88\n\n\n...\n...\n...\n\n\n6512\n4.0\nNaN\n\n\n6513\n0.0\n11.83\n\n\n6514\n5.0\n7.37\n\n\n6515\n6.0\n9.07\n\n\n6516\n12.0\n16.07\n\n\n\n\n6517 rows × 2 columns\n\n\n\n\ns = linear_score(X_test, w)\ndf_test['score'] = s\n\n\nt = 2.9\n\n# compute the scores\ns     = linear_score(X_test, w)\npreds = s &gt;= t\ndf_test[\"preds\"] = preds\n\n# compute error rates\nFPR   = ((preds == 1) & (y_test == 0)).sum() / (y_test == 0).sum()\nTPR   = ((preds == 1) & (y_test == 1)).sum() / (y_test == 1).sum()\n\ncost_of_FN = df_test.loc[df_test[\"preds\"] == False]['cost_if_default'].sum()\ngain_of_TN = df_test.loc[df_test[\"preds\"] == True]['gain_if_repaid'].sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = cost_of_FN*FNR  + gain_of_TN*TNR \ngain\n\n49821552.0501864\n\n\n\ngain/len(df_test)\n\n7644.859912565045\n\n\nLooking at the code above we see something very interesting that using the same weight vector and technique for gain and cost calculations we have a much larger total gain for the chosen threshold. I’m not totally sure why this is, but instead of the previous ~ $8.7 million we are now getting a total gain calculation of ~ $49.8 million which calculates out to be a gain of $7644.86 per loan which seems to me like a crazy profit per loan for the bank to receive. This leads me to believe that my calculations somewhere are incorrect, but I can’t find anything just looking at my code.\n\n\nModel Prediction Bias Analysis\nIt does not look like there is any age group that stands above the rest in being discriminated based on age. Though it does seem like younger ages on average are more negatively affected by the test predictions.\n\ndf_test.groupby(['preds'])['person_age'].mean()\n\npreds\nFalse    27.841419\nTrue     27.584048\nName: person_age, dtype: float64\n\n\n\ntotal_preds = df_test['preds'].sum()\nmedical_preds = df_test.loc[df_test[\"loan_intent\"] == \"MEDICAL\"]['preds'].sum()\nmedical_actual = df_test.loc[df_test[\"loan_intent\"] == \"MEDICAL\"]['loan_status'].sum()\nprint(f'Total Number of predicted Defaults: {total_preds}')\nprint(f'Number of predicted defaults on medical loans: {medical_preds}')\nprint(f'Number of actual Medical loans that have defaulted {medical_actual}')\n\nTotal Number of predicted Defaults: 2683\nNumber of predicted defaults on medical loans: 519\nNumber of actual Medical loans that have defaulted 348\n\n\nIt looks like almost a quarter of the default predictions made by the model are made on the loans with Medical as their intent. However, that is actually 200 more than the actual default status of those loans.\n\ndf_test.groupby('preds')['person_income'].mean()\n\npreds\nFalse    68560.065989\nTrue     63495.321655\nName: person_income, dtype: float64\n\n\nAlthough the difference between the two isn’t that seemingly large. The mean income for individuals predicted not to default is $6000 higher than the mean for individuals that are predicted to default on their loans. So it seems that people with higher incomes are treated better in this model.\n\n\nDiscussion\nThrough this study we were able to see that in a test set there was bias against person income and somewhat against people intending to pay for medical expenses with a loan. Based on the model predictions people with higher incomes were predicted to default on their loans less than people with lower incomes. Something to note is that the profit per person in the test set seemed to be ~7500 which was much higher than in the training set.\nAnswering the question “is it fair for people to have a harder time to get access to credit even though people seeking loans for medical purposes default on their loans statistically more than others”. I don’t think it’s fair that they should have a harder time getting credit for their medical expenses. I believe that fairness is that everyone should have an equal opportunity to do anything, and especially for medical expenses I think people should be able to pay for and get any operation done they need to. Hence why I don’t think it’s fair that people should have less opportunity to get credit even if they’re trying to pay for medial expenses."
  },
  {
    "objectID": "posts/perceptron-blog/perceptronBlog.html",
    "href": "posts/perceptron-blog/perceptronBlog.html",
    "title": "The Perceptron",
    "section": "",
    "text": "https://github.com/elavallee-github/elavallee-github.github.io/blob/6cfd3acf71b795806d596d91983a881c74b2064a/posts/perceptron-blog/perceptron.py\nFor my perceptron.grad() implementation (without mini-batching) I first calculate the score and then simply return 1.0(s_iy_i &lt; 0)yX. This correctly implements the math of the perceptron algorithm as we are performing the perceptron update: w = w + y_ix_i only if s_iy_i &lt; 0 which the first half of our return statement is doing. As for the mini-batching, I declared an alpha value and then calculate k based on the size of the batch that has been supplied to the grad() method. Then I set up a for loop compounding what I call the total grad for each value of the mini-batch, I then average that and multiply it by our learning rate and then return."
  },
  {
    "objectID": "posts/perceptron-blog/perceptronBlog.html#abstract",
    "href": "posts/perceptron-blog/perceptronBlog.html#abstract",
    "title": "The Perceptron",
    "section": "Abstract",
    "text": "Abstract\nThe high level purpose of this blog is to explore the Perceptron, a very basic linear classifier. The perceptron is a useful tool to find a weight value for a classifier while minimizing loss however this can only be accomplished if the data is linearly separable so the applications of this model are quite limited. In the follow experiments I aim to confirm my implementation of the perceptron algorithm and to explore how it fairs when dealing with 2 dimensional data, 2 dimensional non-linearly separable data, and multidimensional data.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe following perceptron data function is taken from Professor Chodrows implementation in the live notes\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\nX, y = perceptron_data(n_points = 50, noise = 0.2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nAbove we show a visualization of the data we are about to apply the perceptron algorithm to. We can see our main training loop below\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nInside our main loop is pretty simple we are calculating the overall loss from our data X and y and then picking a random data point from X and y and passing that to our step function. Inside the step function we will call the perceptron.grad() function mentioned at the beginning of the blog and we will update the weight accordingly. This loop is completed until the loss is 0.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAbove we can see a visualization of the loss over time as the perceptron algorithm runs as we can see there is quite a bit of ups and downs, but in just 20 iterations of the loop we reach a weight value that produces 0 loss.\nThe following code blocks are taken from Professor Chodrow’s - Introduction to Classification: The Perceptron\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(3, 4, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = opt.step(x_i, y_i)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWhile the above graph is a little messy it does show the trend for the separating line being decided based off the changing weight value and how that is related to the loss as the algorithm progresses.\n\nX, y = perceptron_data(n_points = 50, noise = 0.7)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nNow we’re moving on to an experiment that deals with data that is not linearly separable, by increasing the noise in the perceptron data function we can produce a data set that is not linearly separable and see how the algorithm fairs here\n\nfrom perceptron import Perceptron, PerceptronOptimizer\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nmax_iterations = 1000\nj = 0\n\nwhile loss &gt; 0:\n    if j &gt; max_iterations:\n        break\n\n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    j+=1\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n\n\n\n\n\n\n\n\nAfter running our training loop for 1000 iterations we still don’t have a separating line that really tells us anything there are still quite a few orange circles above the line and below\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nA similar trend can be seen in the loss where we bounce up and down without ever reaching 0 even after 1000 iterations\n\nX, y = perceptron_data(n_points = 50, noise = 0.2, p_dims=5)\n\nNow we’re experimenting with a dataset that has five dimensions in its feature matrix instead of just 2, and in doing so we can check if this data set is linearly separable or not depending on if we can apply the perceptron to this data.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nFollowing our main training loop and we can see this data should be linearly separable as we were able to reach a loss score of 0 after training. The loss hovers very close to 0 for quite a bit of time and then jumps down at the end.\n\ndef perceptron_mini_batch_training(X, y, k, max_iterations):\n    # instantiate a model and an optimizer\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    n = X.size()[0]\n\n    k = k\n    ix = torch.randperm(X.size(0))[:k]\n    j = 0\n    max_iterations = max_iterations\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        if j &gt; max_iterations:\n            break\n        \n        # not part of the update: just for tracking our progress    \n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # pick a random data point\n        # i = torch.randint(n, size = (1,))\n        # x_i = X[[i],:]\n        # y_i = y[i]\n        ix = torch.randperm(X.size(0))[:k]\n        X_k = X[ix,:]\n        y_k = y[ix]\n        \n        # perform a perceptron update using the random data point\n        opt.step(X_k, y_k)\n        j += 1\n        \n    # plt.plot(loss_vec, color = \"slategrey\")\n    # plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n    # labs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n    fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n    ax.set(xlim = (-1, 2), ylim = (-1, 2))\n    plot_perceptron_data(X, y, ax)\n    draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n    \n\n\nX, y = perceptron_data(n_points = 50, noise = 0.5)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nk = n\nj = 0\nmax_iterations = 10000\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if j &gt; max_iterations:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    # i = torch.randint(n, size = (1,))\n    # x_i = X[[i],:]\n    # y_i = y[i]\n    ix = torch.randperm(X.size(0))[:k]\n    X_k = X[ix,:]\n    y_k = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_k, y_k)\n    j += 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n\n\n\n\n\n\n\n\nHere we are demonstrating that with a k value set to n we can still converge even when the data is not linearly separable as is the case with the above dataset. This can be seen visually with the line above generally splitting the data evenly and below in the graph of the loss where it gets exteremely close to 0 and doesn’t have any spikes above that.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\n\nX, y = perceptron_data(n_points = 50, noise = 0.2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\nperceptron_mini_batch_training(X, y, k=1, max_iterations = 1000)\n\n\n\n\n\n\n\n\nThe above example shows the loss plot of the perceptron with mini-batching when k = 1, and as we can see it does indeed fair quite similarly to the normal perceptron algorithm.\n\nperceptron_mini_batch_training(X, y, k=10, max_iterations = 1000)\n\n\n\n\n\n\n\n\nNow we can see that in the case of a mini-batch with k=10 the algorithm can still find a separating line in 2d.\n\nRuntime discussion\nThinking about the time complexity of a single iteration of the perceptron algorithm the major things that happen are a loss calculation, the random selection of a data point, and the optimization step. The loss calculation is just simple arithmetic so that will be O(1), the random selection is also constant so O(1), and the optimization step calls loss O(1) and grad which in the case of the normal perceptron is again just arithmetic so it too is O(1). So the basic algorithm has a constant runtime for its iterations making it quite efficient.\nFor the case of mini-batching the only difference is that in my implementation I have a for loop that runs in range k so each iteration will be O(k) so in the best case O(1) and in the worst O(n)"
  },
  {
    "objectID": "posts/perceptron-blog/perceptronBlog.html#discussion",
    "href": "posts/perceptron-blog/perceptronBlog.html#discussion",
    "title": "The Perceptron",
    "section": "Discussion",
    "text": "Discussion\nLooking back at the work done in this blog post we can see that the perceptron is an efficient algorithm to use when dealing with a linearly separable dataset. If the data being worked with is not linearly separable then mini-batching is a possible solution to make the algorithm more effective. However, it still is not perfect and doesn’t always achieve 0 loss. The perceptron is an interesting algorithm, but the scope of the data for which it can be used is quite narrow."
  },
  {
    "objectID": "posts/ReplicationPost/ReplicationStudy.html",
    "href": "posts/ReplicationPost/ReplicationStudy.html",
    "title": "Replication Study",
    "section": "",
    "text": "Abstract\nIn this notebook we aim to experiment with the recreation of figures and findings from an article by Obermeyer et al (2019). The work is done to analyze an algorithm that gave black patients lower scores based on the thought that black patients were healthier than white patients due to less money being spent on them. In this work I was able to successfully recreate figures 1 and 3 and eventually produce similar findings regarding the relationship between medical expenditures for black and white patients.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\nrs_male = df.loc[df['dem_female'] == 0]\nrs_female = df.loc[df['dem_female'] == 1]\n\n\n\nFigure 1 Recreation\n\nrs_male = rs_male.copy()\nrs_male_b = rs_male.loc[rs_male['race'] == 'black']\nrs_male_w = rs_male.loc[rs_male['race'] == 'white']\nrs_male_b = rs_male_b.copy()\nrs_male_w = rs_male_w.copy()\nrs_male_b['risk_percent'] = (rs_male_b['risk_score_t'].rank(pct=True) * 100).round()\nrs_male_w['risk_percent'] = (rs_male_w['risk_score_t'].rank(pct=True) * 100).round()\nrs_male_b['mean_chronic'] = rs_male_b.groupby('risk_percent')['gagne_sum_t'].transform('mean')\nrs_male_w['mean_chronic'] = rs_male_w.groupby('risk_percent')['gagne_sum_t'].transform('mean')\n\n\nrs_female = rs_female.copy()\nrs_female_b = rs_female.loc[rs_female['race'] == 'black']\nrs_female_w = rs_female.loc[rs_female['race'] == 'white']\nrs_female_b = rs_female_b.copy()\nrs_female_w = rs_female_w.copy()\nrs_female_b['risk_percent'] = (rs_female_b['risk_score_t'].rank(pct=True) * 100).round()\nrs_female_w['risk_percent'] = (rs_female_w['risk_score_t'].rank(pct=True) * 100).round()\nrs_female_b['mean_chronic'] = rs_female_b.groupby('risk_percent')['gagne_sum_t'].transform('mean')\nrs_female_w['mean_chronic'] = rs_female_w.groupby('risk_percent')['gagne_sum_t'].transform('mean')\n\n\nrs_male = pd.concat([rs_male_b, rs_male_w])\nrs_female = pd.concat([rs_female_b, rs_female_w])\n\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, axs = plt.subplots(1, 2, figsize=(8,3), sharey=True)\n\nsns.scatterplot(ax=axs[0], data=rs_male[['risk_percent', 'mean_chronic', 'race']], x='mean_chronic', y='risk_percent', hue='race', style='race', palette='muted')\nsns.scatterplot(ax=axs[1], data=rs_female[['risk_percent', 'mean_chronic', 'race']], x='mean_chronic', y='risk_percent', hue='race', style='race', palette='muted')\naxs[0].set_xlabel(\"Mean number of chronic illnesses\")\naxs[1].set_xlabel(\"Mean number of chronic illnesses\")\naxs[0].set_title(\"Men\")\naxs[1].set_title(\"Female\")\naxs[0].set_ylabel(\"Percentile Risk Score\")\n\nText(0, 0.5, 'Percentile Risk Score')\n\n\n\n\n\n\n\n\n\n\nPlot Discussion\nThe following plots show the difference between black and white patients in regard to their percentile risk score compared to their average number of chronic illnesses. From looking at both plots within male and female populations white patients have a higher likelihood of getting treatment as they have higher risk scores at similar chronic illness levels.\n\n\n\nFigure 3 Recreation\n\nrs_b = df.loc[df['race'] == 'black']\nrs_w = df.loc[df['race'] == 'white']\nrs_b = rs_b.copy()\nrs_w = rs_w.copy()\nrs_b['risk_percent'] = (rs_b['risk_score_t'].rank(pct=True) * 100).round()\nrs_w['risk_percent'] = (rs_w['risk_score_t'].rank(pct=True) * 100).round()\nrs_b['total_medical_cost'] = rs_b.groupby('risk_percent')['cost_t'].transform('mean')\nrs_w['total_medical_cost'] = rs_w.groupby('risk_percent')['cost_t'].transform('mean')\nrs = pd.concat([rs_b, rs_w])\n\n\nrs_b['illness_cost'] = rs_b.groupby('gagne_sum_t')['cost_t'].transform('mean')\nrs_w['illness_cost'] = rs_w.groupby('gagne_sum_t')['cost_t'].transform('mean')\nrs = pd.concat([rs_b, rs_w])\n\n\nrs\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\nrisk_percent\ntotal_medical_cost\nillness_cost\n\n\n\n\n8\n1.087141\n0\n1000.0\n0.0\n118.0\n5.800000\n39.1\n0.490\n78.0\nblack\n...\n0\n1\n1\n0\n1\n1\n1\n17.0\n229100.0\n6783200.0\n\n\n20\n11.092237\n0\n2900.0\n0.0\n124.0\nNaN\nNaN\nNaN\nNaN\nblack\n...\n0\n0\n0\n0\n0\n4\n2\n90.0\n547200.0\n6172200.0\n\n\n23\n0.611517\n0\n1900.0\n400.0\n106.0\nNaN\nNaN\nNaN\nNaN\nblack\n...\n0\n1\n0\n0\n1\n0\n0\n8.0\n56700.0\n5799000.0\n\n\n30\n4.552404\n0\n1900.0\n100.0\n140.0\n6.233333\n40.8\n1.000\n81.0\nblack\n...\n0\n0\n1\n0\n0\n1\n1\n65.0\n324400.0\n6783200.0\n\n\n47\n1.885510\n0\n2000.0\n0.0\n123.0\nNaN\n37.9\n0.955\nNaN\nblack\n...\n0\n0\n0\n0\n0\n1\n1\n32.0\n202200.0\n6783200.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48779\n0.611517\n0\n800.0\n0.0\nNaN\nNaN\nNaN\n1.090\n148.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n7.0\n957800.0\n89457400.0\n\n\n48780\n2.615933\n0\n2200.0\n0.0\n112.0\nNaN\n41.4\n0.810\n172.0\nwhite\n...\n0\n1\n0\n0\n1\n1\n1\n46.0\n2456500.0\n63470100.0\n\n\n48781\n1.358926\n0\n800.0\n0.0\n105.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n1\n0\n0\n1\n1\n0\n23.0\n1989700.0\n89457400.0\n\n\n48782\n10.990318\n0\n1300.0\n0.0\n132.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n3\n3\n94.0\n6924300.0\n33040500.0\n\n\n48783\n1.681671\n0\n4400.0\n0.0\n115.0\n5.600000\n36.6\n0.940\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n30.0\n1519100.0\n89457400.0\n\n\n\n\n48784 rows × 163 columns\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(8,4), sharey=True)\n\nsns.scatterplot(ax=axs[0], data=rs[['risk_percent', 'total_medical_cost', 'race']], x='risk_percent', y='total_medical_cost', hue='race', style='race')\nsns.scatterplot(ax=axs[1], data=rs[['illness_cost', 'gagne_sum_t', 'race']], x='gagne_sum_t', y='illness_cost', hue='race', style='race')\naxs[0].set_yscale('log')\naxs[0].set_ylabel('Mean Total Medical Expenditures')\naxs[0].set_xlabel('Percentile Risk Score')\naxs[1].set_xlabel('Number of Chronic Illnesses')\n\nText(0.5, 0, 'Number of Chronic Illnesses')\n\n\n\n\n\n\n\n\n\n\nModel Discussion\nLooking at the above plot we can see that generally total expenditures increase with the number of chronic illnesses and percentile risk score. However it seems that at lower chronic illness amounts white patients have higher medical expenses than black patients. This is especially interesting given that the major of patients in the study have 5 or fewer illnesses.\n\n\n\nModeling Cost Disparity\n\nlen(df.loc[df['gagne_sum_t'] &lt;= 5])/len(df)\n\n0.9553952115447688\n\n\nIt looks like 95% of the data is made up of patients with 5 or fewer chronic conditions.\n\ndf_sub = df.drop(df[df['cost_t'] == 0].index)\n\n\nimport numpy as np\ndf_sub['log_transform'] = np.log(df['cost_t'])\n\n/Users/ethanlavallee/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n\n\n\n# from sklearn.preprocessing import LabelEncoder\n# le = LabelEncoder()\n# le.fit(df_sub[\"race\"])\n\n# df_sub['encoded_race'] = le.transform(df_sub[\"race\"])\ndf_sub['encoded_race'] = df_sub.apply( lambda df_sub: (0 if df_sub['race'] == 'white' else 1), axis=1)\ndf_sub\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\nlog_transform\nencoded_race\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n7.090077\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n0\n1\n0\n0\n1\n4\n3\n7.863267\n0\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n6.214608\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n7.170120\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n1\n1\n7.003065\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48779\n0.611517\n0\n800.0\n0.0\nNaN\nNaN\nNaN\n1.090000\n148.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n6.684612\n0\n\n\n48780\n2.615933\n0\n2200.0\n0.0\n112.0\nNaN\n41.4\n0.810000\n172.0\nwhite\n...\n0\n0\n1\n0\n0\n1\n1\n1\n7.696213\n0\n\n\n48781\n1.358926\n0\n800.0\n0.0\n105.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n1\n0\n0\n1\n1\n0\n6.684612\n0\n\n\n48782\n10.990318\n0\n1300.0\n0.0\n132.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n3\n3\n7.170120\n0\n\n\n48783\n1.681671\n0\n4400.0\n0.0\n115.0\n5.6\n36.6\n0.940000\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n8.389360\n0\n\n\n\n\n46887 rows × 162 columns\n\n\n\n\nX = df_sub[['encoded_race', 'gagne_sum_t']]\ny = df_sub['log_transform']\n\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\n\nX1 = add_polynomial_features(X, 1)\nX2 = add_polynomial_features(X, 2)\nX3 = add_polynomial_features(X, 3)\nX4 = add_polynomial_features(X, 4)\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nLR = LinearRegression()\n\nfor i in range(10):\n    Xn = add_polynomial_features(X,i)\n    LR.fit(Xn, y)\n    cv_scores_LR = cross_val_score(LR, Xn, y, cv = 5)\n    print(f'{cv_scores_LR.mean()} {i}')\n\n\n0.14538846793594346 0\n0.14538846793594346 1\n0.14537952700038775 2\n0.14699336604386143 3\n0.146909632661908 4\n0.1473340656385787 5\n0.14776094174784324 6\n0.1480739986116853 7\n0.14811660549076067 8\n0.14820529985520275 9\n\n\n\nLR.fit(add_polynomial_features(X, 3), y)\nLR.coef_\n\narray([-2.66242046e-01, -2.04644178e+10,  2.04644178e+10, -7.24885394e-03])\n\n\n\nnp.e**LR.coef_[0]\n\n0.7653993676474179\n\n\nThe cost incurred by Black patients is about 77% that of White patients. Which tracks with the findings in the Obermeyer et al (2019). paper which says that there is a bias in the data where there is a correlation between race and cost where Black patients generate lower costs than Whites.\n\n\nDiscussion\nFrom the experimentation done in this work, we were able to recreate the figures and findings produced by Obermeyer et al (2019). Through this work I was able to better understand the work that was done by the researchers and to become more confident organizing data into interpretable plots. A major issue in the algorithm was that is was using medical cost as a proxy for health. While it seemed at first that this was actually a fine decision it was found that this approach was improperly calibrated, so improper on the basis of sufficiency. The lack in sufficiency for the model didn’t take into account that black patients had less money spent on them which led to an assumption that black patients were healthier."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Logistic Regression\n\n\n\n\n\nLogistic Regression Blog Post\n\n\n\n\n\nApr 7, 2024\n\n\nEthan Lavallee\n\n\n\n\n\n\n\n\n\n\n\n\nThe Perceptron\n\n\n\n\n\nThe Perceptron Blog Post\n\n\n\n\n\nApr 4, 2024\n\n\nEthan Lavallee\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study\n\n\n\n\n\nReplication Study Blog Post\n\n\n\n\n\nMar 7, 2024\n\n\nEthan Lavallee\n\n\n\n\n\n\n\n\n\n\n\n\nWho’s Cost?\n\n\n\n\n\nWho’s Cost? Blog Post\n\n\n\n\n\nFeb 29, 2024\n\n\nEthan Lavallee\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nPalmer Penguins Blog Post\n\n\n\n\n\nFeb 21, 2024\n\n\nEthan Lavallee\n\n\n\n\n\n\nNo matching items"
  }
]